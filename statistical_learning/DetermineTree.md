## 统计机器学习算法整理

    整理思路

    《统计学习方法》过一遍，把从书上提炼出来的东西做一下整合和展
    开，可以结合一些之前做过的作业和总结的资料；最后看一遍cs229查漏补缺

注：这个文档目的是串起各参考资料，以及对一些细节问题的补充说明；未来加入细节问题的超链接

----

- 大纲

1. 决策树

1. 集成方法

### 1、决策树

- 概念注意要点：
在分类问题中，决策树表示基于特征对实例进行分类的过程，可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。

- 学习注意要点：
决策树的损失函数通常是正则化的极大似然函数。而从所有可能的决策树中选取最优决策树是NP完全问题，因此一般采用启发式方法，得到的决策树是次最优的。（详见李航57页）

- 不纯度的度量（详见手写pdf）
  - Gini指数
  - 误分率
  - 熵
  - 信息增益
  - 信息增益比（详见李航63页）

- 何时停止的问题
  - ID3算法介绍：
    通过设置阈值进行停止；ID3算法的核心是在树的各个节点上应用**信息增益准则**选择特征，递归地构建决策树；ID3相当于用极大似然法进行概率模型的选择。
    且ID3算法只有树的生成，容易产生过拟合。
  
  - C4.5算法
    最明显的改进是利用信息增益比来进行特征选择，避免了信息增益可能会产生的小而均匀的分割。且C4.5可以处理缺失值、连续变量并**剪枝**
    - 剪枝策略：
      - 决策树的剪枝往往通过极小化决策树整体的损失函数来实现（详见李航65-67页）。
      - 决策树的整体损失函数：$$C_a(T) = \sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|.$$
      其中第一项是训练数据的预测误差，第二项代表树模型的复杂度。

  - CART算法介绍（详见李航67页）
    - 分类回归树：既可以处理分类问题也可以处理回归问题
    - 决策树生成：充分生长
      - 回归树
      - 分类树
    - 决策树剪枝
      - 与前面的C4.5剪枝有不同
    - 优势：详见手写pdf。
    - Question
      - CART中的自变量可以是分类变量和连续变量的混合，CART具体是怎么处理的？
      - A：我觉得可以按照连续或者离散的形式去切分，然后看Gini指数进行特征选择。

- `拓展`
  - 详见手写pdf最后。
    

- 参考资料
[1] 李航《统计学习方法》
[2] 大数据挖掘课上的笔记pdf
[3] 薛薇ppt

### 2、集成方法

集成方法有助于提高模型预测精度和稳健性
源于样本均值抽样分布的思路：$var(\bar{x})=\frac{\sigma^2}{n}.$

**重点**：如何获得多个独立样本，以及如何组合多个模型
**常见技术**：Bagging、Boosting等（见薛薇ppt）

#### Boosted Tree

XGBoost基本知识：薛薇ppt

XGBoost深入理解，好在哪？（陈劲松ppt）

xgboost GBDT LightGBM 区别 (陈劲松ppt)
总结：

`共同的点`
  - 梯度方法，每一步利用伪残差进行新树的生成；
  - 每一步可以有一个shrinking参数，控制学习的步长
  - 可以对数据集进行下采样，提高运行速度

`XGBoost与GBDT的区别`
  - 基分类器的选择
    - 支持线性分类器，而不仅仅是CART
  - 泰勒展开到二阶
    - 支持自定义损失函数，只要一阶和二阶可导
  - 偏差-方差权衡
  - 列抽样
  - XGBoost在特征力度上支持并行
  - 其他
    - XGBoost在利用贪心策略寻找分裂点方面利用了近似
    - 允许数据存在缺失

`LightGBM注意的点`
  - 占用资源少，速度快
  - 直方图优化
  - 深度限制的节点分裂方法
  - 原生支持分类型的自变量，而不用事先one-hot encoding.
  - 支持多种不同方式的并行
  - 在一些计算细节上做了优化：比如梯度单边采样计算信息增益，独立特征合并等
  

xgboost是如何分裂节点的，其他算法也要注意过程中的关键环节实施


- 参考资料
[1] 陈劲松ppt

---

以及树模型是如何筛选变量的

为什么L正则可以防止过拟合