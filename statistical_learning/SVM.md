## 统计机器学习算法整理

    整理思路

    《统计学习方法》过一遍，把从书上提炼出来的东西做一下整合和展
    开，可以结合一些之前做过的作业和总结的资料；最后看一遍cs229查漏补缺

----

- 大纲

1. 感知机

1. SVM

1. 神经网络

### 1、感知机

感知机是二分类的线性判别模型，是神经网络和支持向量机的基础。

这里和之后所涉及的内容都是二分类问题，输入为实例的特征向量，输出空间是表示实例的类别，取二值+1和-1，因此我们在这里统一给定训练集的具体形式，之后不再给出：

$$T=\{ (x_1, y_1),(x_2,y_2),..., (x_N,y_N)\}$$

其中，$x_i \in X = R^n, y_i \in Y = \{ +1,-1\}, i=1,2,...,N.$

**模型**：

$$f(x)=sign(w \cdot x+b)$$

模型的假设空间为定义在特征空间中的所有线性分类模型或线性分类器，即函数集合$\{ f|f(x)=w \cdot x+b \}.$

分离超平面：$w \cdot x+b=0$

**策略**：

损失函数的一个自然选择是误分类点的总数，即所谓的0-1损失函数，但是这样的损失函数**不是参数w,b的连续可导函数**，不易于优化，因此感知机选择误分类点到超平面的总距离作为损失函数。如图所示，虚线是感知机所选择的损失函数。（实线是SVM的损失函数，后面讲）

@import "pictures/lossfunction.png" {width="400px" height="200px" title="损失函数" alt="我的 alt"}

由于单个点到分离超平面$w \cdot x + b = 0$的距离是$\frac{1}{||w||} |w \cdot x_0 + b|$，其中$||w||$是w的$L_2$范数，所以可以写出感知机的损失函数为$$L(w,b) = - \sum_{x_i \in M} y_i (w \cdot x_i + b)$$

其中$M$为误分类点的集合，这个损失函数就是感知机学习的经验风险函数。

**算法**：

接下来需要求解损失函数的最优化问题，选择的方法是`随机梯度下降法`。

需要知道，当训练数据线性可分时，感知机算法存在许多个解，这些解依赖于初值的选择和迭代过程中误分类点的选择顺序；而当数据线性不可分时，感知机学习算法不收敛，迭代结果会发生震荡。

----

**对偶形式**


**总结**：

了解感知机的算法，尤其是损失函数部分，理解为什么其采用SGD，以及解不唯一的原因。

### 2、SVM

- 希尔伯特空间：完备的内积空间，是有限维欧式空间的推广，使之不局限于实数的情形和有限的维数，但又不失完备性。

- 支持向量机的输入输出空间：

  - **输入空间**：欧式空间或离散集合

  - **特征空间**：欧式空间或者希尔伯特空间

输入都是由输入空间转换到特征空间，所以支持向量机的学习是在特征空间进行的。

#### （1）线性可分支持向量机

- 与感知机的不同之处：感知机利用误分类最小的策略，求得分离超平面，解有无穷多个，而线性可分支持向量机利用间隔最大化求最优分离超平面，解是唯一的。

- 函数间隔$\hat{\gamma}$ & 几何间隔$\gamma$：
$$\gamma_i = \frac{\hat{\gamma_i}}{||w||}$$ $$\gamma = \frac{\hat{\gamma}}{||w||}$$

超平面关于样本点的几何间隔一般是实例点到超平面的带符号的距离。

- 间隔最大化：**是指让间隔最小的点到超平面的间隔也最大，而不是所有点的平均距离最大**。

@import "pictures/margin.png" 

- 最优化问题

@import "pictures/SVM1.png"

- 支持向量和间隔边界

在决定分离超平面时只有支持向量起作用，而其他实例点并不起作用。

#### <font color=red>学习的对偶算法</font>

参考：

《统计学习方法》附录C

[拉格朗日对偶性及KKT条件推导](https://www.jianshu.com/p/19c2ed0259a8)

`推导`

#### （2）线性支持向量机与软间隔最大化

- 线性支持向量机

理解线性不可分的含义，以及引入松弛变量为了解决的问题。并结合支持向量进一步理解。

@import "pictures/SVM2.png"

可以证明$w$的解是唯一的，但$b$的解不唯一。

- 对偶问题

`推导`

注意由于$b$的解不唯一，因此实际计算时可以取所有解的均值。

- <font color=red>支持向量</font>

@import "pictures/soft margin.png"

@import "pictures/soft margin2.png"

- <font color=red>合页损失函数</font>


@import "pictures/lossfunction.png"

线性支持向量机的另一种解释是优化如下目标函数：

$$\sum_{i=1}^{N} [1-y_i (w \cdot x_i + b)]_+ + \lambda ||w||^2$$

可以认为线性支持向量机是优化由0-1损失函数的上界（合页损失函数）构成的目标函数。

<font color=red>从这个角度来看，$\frac{1}{2}||w||^2$是作为一个正则项存在，该怎么理解？</font>

#### （3）非线性支持向量机与核函数

核技巧应用到支持向量机，其基本想法就是通过一个非线性变换将输入空间（欧式空间或离散集合）对应于一个特征空间（希尔伯特空间），使得在输入空间中的超曲面模型对应于特征空间中的超平面模型（支持向量机），这样，分类问题的学习任务通过在特征空间中求解线性支持向量机就可以完成。

- 核函数

- 核技巧在支持向量机中的应用——非线性支持向量机

- `正定核`

- `常用核函数`

#### （4）序列最小最优化算法

`推导`

#### （5）拓展

1. 核技巧应用于其他线性模型

1. SVM的正则化

1. 手写数字识别

### 3、神经网络

- 手写pdf、gnn ppt
