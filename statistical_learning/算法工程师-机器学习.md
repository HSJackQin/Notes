# 机器学习算法必备知识点

> 依据《百面机器学习》总结

## 一、特征工程

### 1、特征归一化

- 特征归一化的方法

  - 线性函数归一化（min max scaling）

  - 零均值归一化（Z-Score Normmalization）

- 为什么要进行归一化
  
  - 归一化使得特征之间处于同一数量级，具有可比性

  - 归一化有利于梯度下降法的更快收敛（等值图解释）

  - 因此，通过随机梯度下降法求解的模型（LR，SVM，DNN）等往往需要进行归一化，而基于信息增益进行节点分裂的树模型则不需要归一化

### 2、类别特征

- 类别特征的编码方式

  - 序号编码，独热编码，二进制编码等

### 3、组合特征

- 为什么要引入组合特征？
  
  - 为了提高模型对复杂关系的表征能力

- 如何处理高维组合特征

  - 降维

  - 构造出有效的组合特征，而不是全部构造

### 4、文本表示模型

- 词袋模型

  - 忽略词的先后顺序和上下文，直接进行长向量编码

  - 权重可以使用TF-IDF得分

- N-gram模型

  - 将相邻的n个词共同作为一个特征放进向量表示中

- 主题模型

  - 从文本库中发现主题，得到每个主题上面词的分布，并计算每篇文章的主题分布

- 词嵌入与深度学习模型

  - 将每个词映射成低维空间上的一个稠密向量，然后通过CNN，RNN等进行特征的表征，抽取出一些高层的语义特征。

  - 同全连接层相比，CNN，RNN等一方面很好的抓住了文本的特性，另一方面减少了网络的学习参数，提高训练速度，降低过拟合。

- Word2Vec

  - 最常用的词嵌入模型

  - 浅层神经网络模型

    - CBOW

    - Skip-gram

- LDA模型

### 5、训练集的处理

- 样本不平衡/样本不够

  - 思路一：增加模型的先验

    - 简化模型，添加惩罚等，使得模型更加鲁棒

  - 思路二：想办法增加样本

    - 数据集增强：各种变换，GAN等

    - 在特征空间上的变换，比如SMOTE等

    - 深度学习中还可以采用迁移学习，获取预训练模型

## 二、模型评估

### 1、模型评估

- 不同问题（分类，排序，回归）往往采用不同的指标进行评估，且要采用不止一种。

  - 比如P-R曲线/F1-score，权衡不同场景下模型的精确和召回

  - 要注意离群点对回归模型RMSE的影响

### 2、ROC曲线

- 横坐标为假阳性率（FPR），是指负样本中被错判为正样本的比例，纵坐标为真阳性率（TPR），是指真正的正样本被正确找到的比例。

- ROC曲线&P-R曲线

  - 优势：ROC不易受到样本不平衡变动的干扰，因此对数据集有较好的鲁棒性。

### 3、特征相似度的计算

- 欧式距离

- 余弦相似度

$$
\cos (A, B)=\frac{A \cdot B}{\|A\|_{2}\|B\|_{2}}
$$

- KL距离（相对熵）

  - 用于计算两个分布之间的差异

  - 不满足对称性和三角不等式，因此不符合真正的距离的定义

### 4、AB测试

- 在对模型进行了充分的离线评估之后，为什么还需要AB测试？

  - （1）离线评估无法完全消除模型过拟合的影响

  - （2）离线评估无法完全还原线上环境

  - （3）线上系统的某些商业指标在离线评估中无法计算，比如点展，留存，PV访问量等

- 如何进行AB测试？

  - 核心：对用户进行分桶：划分实验组和对照组，保证样本的独立性和采样方式的无偏性。

### 5、模型评估

- Holdout

- 交叉验证

- bootstrap

### 6、超参数调优

- 网格搜索

- 随机搜索

- 贝叶斯优化

### 7、过拟合与欠拟合

- 过拟合的含义

  - 模型过于复杂，对训练数据的拟合过当（学习了噪声的特征），导致训练集表现很好，但测试集表现很差。

- 降低过拟合的方法

  - more训练数据

  - 降低模型复杂度

  - 正则化

  - 集成学习

- 降低欠拟合的方法

  - 添加新特征

  - 增加模型复杂度

  - 减小正则化系数

## 三、经典算法

### 1、SVM

### 2、LR（逻辑回归）

- 逻辑回归的损失函数为什么要用极大似然

- 逻辑回归的求解
  
  - 常规：梯度下降

  - 改进：基于二阶近似的牛顿法（BFGS和LBFGS），以及解决流处理问题的随机梯度下降

### 3、决策树

## 四、降维

### 1、主成分分析

- 线性，非监督，全局

- 信噪比越大意味着数据的质量越好

- PCA目标：**最大化投影方差，即让数据在主轴上投影的方差最大。**

## 五、非监督学习

非监督学习的输入数据没有标签，需要通过算法模型来挖掘数据内在的结构和模式。非监督学习主要包含两大类学习方法：**数据聚类**和**特征变量关联**。

### 1、数据聚类

#### <1> K-Means

#### <2> GMM（高斯混合模型）

### 2、线性判别分析

### 3、其他

- 等距映射，局部线性嵌入，拉普拉斯特征映射，局部保留投影等

## 六、概率图模型

## 七、优化算法

机器学习算法 = 模型表征 + 模型评估 + 优化算法

例：

（1）LR = 线性分类模型 + 交叉熵 + 随机梯度下降

（2）SVM = 线性分类模型 + 最大间隔 + SMO等求解方法

### 1、损失函数

- 0-1损失函数及其代理损失函数

  - Hinge损失函数
    - 0-1损失的凸上界，比较紧
  - Logistic损失函数
    - 0-1损失的光滑的凸上界
  - 交叉熵损失函数
    - 也是光滑的，且规范在[-1,1]之间
  - 平方损失函数

  - 绝对损失函数（对异常点更加鲁棒）

  - Huber损失函数

### 2、凸优化和非凸优化

- 逻辑回归，支持向量机等为凸优化问题，非凸优化包括低秩模型（如矩阵分解），深度神经网络模型等。

### 3、经典优化算法

- （1）直接求解
  
  - 需要满足：<1>凸函数 <2>有闭式解

  - 比如岭回归

- （2）迭代求解

  - 一阶近似：梯度下降法

  - 二阶近似：牛顿法

### 4、随机梯度下降

- 随机梯度下降的两个困难：
  
  - 山谷

  - 鞍点

- 解决方案

  - 动量法

  - AdaGrad方法：对参数步长进行实时更新

  - Adam方法：

### 5、L1正则和稀疏性



## 八、采样

## 九、前向神经网络

- 神经网络中的BN层

- 神经网络徒手实现

- BP更新

## 十、循环神经网络

## 十一、强化学习

## 十二、集成学习

- 从偏差方差的角度理解Boosting和Bagging

  - Boosting是从降低偏差的角度来优化模型

  - Bagging是从减小方差的角度来得到更好的模型的

- 集成学习框架

  - adaboost

  - GBDT

  - voting or stacking ?

  - 为什么基分类器一般选择树模型？

    - （1）决策树可以方便的将样本权重整合到训练过程中，而不需要使用过采样方法来调整样本权重。

    - （2）决策树的表达能力和泛化能力，可以通过调节树的层数来折中。

    - （3）决策树是不稳定的分类器，且选择特征分裂时可以很好的引入随机性

    - 另一个适合做基分类器的模型是神经网络

  - 随机森林的基分类器，可不可以由线性分类器或者k近邻来代替？

    - 不可以，因为bagging框架需要基分类器方差大，这样才有好的提升作用

### GBDT

- 框架

- 梯度提升和梯度下降的区别和联系

  - 梯度提升可以看做是框架化的梯度下降，它可以将梯度下降应用到不同的函数模型中


- XGBoost和GBDT有什么区别

## 十三、生成对抗网络

## 十四、人工智能的热门应用